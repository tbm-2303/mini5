# mini5

• Did you learn anything new about the human’s freedom?
  Sure! We learned alot about differnt metrics for human freedom around the world.
  
• How many categories of freedom (clusters) have you defined?
  It seems like the clustering models prefered 2 cluster except for mean-shift, which is also the model we prefered. this model prefered 4 clusters.
  
• Which clustering methods and algorithms have you implemented?
K-means
Hierarchical 
Mean Shift

• What motivates your choice of algorithms?
- Data Type: Different clustering algorithms are designed for specific types of data. For example, K-means is suitable for numerical data, while categorical data may require algorithms like k-prototype clustering.
- Scalability: Consider the size of your dataset. Some algorithms are more scalable than others. K-means, for instance, can handle large datasets better than hierarchical clustering.
- Computational Complexity: Depending on the size of your data and computational resources, you may need to choose an algorithm that suits your constraints.
- Cluster Shape: K-means assumes spherical clusters. If your clusters have non-spherical shapes, algorithms like DBSCAN or Gaussian Mixture Models (GMM) might be more appropriate.
- Cluster Density: Some algorithms can handle clusters of varying shapes and densities. DBSCAN, for example, can find clusters with different shapes and densities.
- Interpretability of Results: Consider the interpretability of the results. K-means provides easily interpretable results, while hierarchical clustering can show a more detailed hierarchy.
- Outlier Sensitivity: Consider how the algorithm handles outliers. Some algorithms, like K-means, can be sensitive to outliers, while DBSCAN is more robust.
- demintionality. Primary Component Analysis(PCA) could be needed to reduce the dimentionality of the data.
- Also, we just want to familiarize us with the theory!

K-Means:

Strengths:
Efficient and scalable, especially for large datasets.
Works well with spherical clusters.
Simple and easy to understand.
Suitable for:
Numerical data.
Well-separated, equally sized, and roughly spherical clusters.


Hierarchical Clustering:

Strengths:
Provides a hierarchy of clusters, useful for understanding data at different granularities.
No need to specify the number of clusters in advance.
Suitable for:
Datasets with a nested cluster structure.
Cases where the hierarchy of clusters is important.


DBSCAN (Density-Based Spatial Clustering of Applications with Noise):

Strengths:
Can find clusters of arbitrary shapes.
Robust to outliers.
Does not require specifying the number of clusters.
Suitable for:
Data with irregular shapes and varying densities.
Datasets with noise and outliers.


Gaussian Mixture Model (GMM):

Strengths:
Assumes that the data is generated by a mixture of Gaussian distributions.
Can model clusters with different shapes and sizes.
Suitable for:
Data with underlying Gaussian distribution.
Situations where clusters have different variances.


Agglomerative Clustering:

Strengths:
Produces a hierarchy of clusters, allowing for different granularities.
Can handle different distance metrics.
Suitable for:
Datasets with a nested cluster structure.
Cases where the hierarchy of clusters is important.


OPTICS (Ordering Points To Identify the Clustering Structure):

Strengths:
Similar to DBSCAN but does not require specifying the neighborhood size.
Reveals the density-based clustering structure.
Suitable for:
Data with varying densities.
Situations where automatic determination of the neighborhood size is preferred.


K-Medoids:

Strengths:
Robust to outliers compared to K-means.
Works well with non-Euclidean distances.
Suitable for:
Categorical data or situations where using medoids (data points representing clusters) is preferred over means.


Spectral Clustering:

Strengths:
Effective for non-convex clusters.
Can capture complex relationships between data points.
Suitable for:
Data with clear spectral properties.
Situations where the data has a low-dimensional structure.


Fuzzy C-Means:

Strengths:
Allows data points to belong to multiple clusters with different degrees of membership.
Suitable for:
Situations where a data point can belong to multiple clusters simultaneously.
Cases where uncertainty in cluster assignment is present.







We were not able to use the folium library. We ran into some trouble with this was'nt able to color the map based on the clusters. We would love to over the process of using folium with the other group on thuesday in the break or whenever. 



